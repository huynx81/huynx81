{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langchain-nvidia-ai-endpoints langchain langchain-community schedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_nvidia_ai_endpoints import ChatNVIDIA\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "import schedule\n",
    "import time\n",
    "\n",
    "# Set NVIDIA API key\n",
    "os.environ[\"NVIDIA_API_KEY\"] = \"your-nvidia-api-key\"\n",
    "\n",
    "# Initialize NVIDIA LLM\n",
    "llm = ChatNVIDIA(\n",
    "    base_url=\"http://your-nvidia-endpoint-url:8000/v1\",  # Replace with your NVIDIA endpoint\n",
    "    model=\"meta/llama3-8b-instruct\"\n",
    ")\n",
    "\n",
    "# Define prompt template\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"Summarize the following content in 2-3 sentences:\\n\\n{content}\"\n",
    ")\n",
    "\n",
    "# Create chain\n",
    "chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "def summarize_website(url=\"https://example.com\"):\n",
    "    try:\n",
    "        # Load website content\n",
    "        loader = WebBaseLoader(url)\n",
    "        docs = loader.load()\n",
    "        content = docs[0].page_content\n",
    "\n",
    "        # Get summary\n",
    "        summary = chain.invoke({\"content\": content})\n",
    "        print(f\"Summary for {url}:\\n{summary}\\n\")\n",
    "\n",
    "        # Optional: Save to file\n",
    "        with open(\"summary.txt\", \"a\") as f:\n",
    "            f.write(f\"{time.ctime()} - {url}:\\n{summary}\\n\\n\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error summarizing {url}: {str(e)}\")\n",
    "\n",
    "# List of websites to summarize\n",
    "websites = [\n",
    "    \"https://example.com\",\n",
    "    \"https://news.example.org\",\n",
    "    # Add more URLs as needed\n",
    "]\n",
    "\n",
    "def daily_summary_job():\n",
    "    print(f\"Starting daily summary at {time.ctime()}\")\n",
    "    for url in websites:\n",
    "        summarize_website(url)\n",
    "\n",
    "# Schedule daily task at 8:00 AM\n",
    "schedule.every().day.at(\"08:00\").do(daily_summary_job)\n",
    "\n",
    "# Keep script running\n",
    "while True:\n",
    "    schedule.run_pending()\n",
    "    time.sleep(60)  # Check every minute"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ".env file\n",
    "\n",
    "# --- Google AI ---\n",
    "GOOGLE_API_KEY=\"YOUR_GOOGLE_API_KEY\"\n",
    "GOOGLE_MODEL_NAME=\"gemini-1.5-flash-latest\" # Or gemini-pro, etc.\n",
    "\n",
    "# --- NVIDIA NIM ---\n",
    "# Get from NGC Console after deploying a model NIM\n",
    "NVIDIA_API_KEY=\"YOUR_NVIDIA_API_KEY\"\n",
    "# The Invoke URL from your NIM deployment details (remove /v1/chat/completions)\n",
    "NVIDIA_BASE_URL=\"YOUR_NVIDIA_ENDPOINT_BASE_URL\"\n",
    "# The model name your NIM is serving (check NIM documentation/deployment)\n",
    "NVIDIA_MODEL_NAME=\"meta/llama3-70b-instruct\" # Example, use your deployed model\n",
    "\n",
    "# --- Ollama ---\n",
    "# Optional: If Ollama runs on a different host/port\n",
    "# OLLAMA_BASE_URL=\"http://localhost:11434\"\n",
    "OLLAMA_MODEL=\"llama3\" # Or mistral, etc. (ensure it's pulled: ollama pull llama3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langchain langchain-google-genai langchain-nvidia-ai-endpoints langchain-community python-dotenv beautifulsoup4 html2text\n",
    "\n",
    "import os\n",
    "import logging\n",
    "from typing import Literal, Optional\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.documents import Document\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_nvidia_ai_endpoints import ChatNVIDIA\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_core.language_models.chat_models import BaseChatModel\n",
    "\n",
    "# --- Configuration & Setup ---\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "load_dotenv()\n",
    "\n",
    "# --- LLM Provider Selection ---\n",
    "\n",
    "def get_llm(provider: Literal['google', 'nvidia', 'ollama']) -> Optional[BaseChatModel]:\n",
    "    \"\"\"Initializes and returns the specified LangChain Chat Model.\"\"\"\n",
    "    try:\n",
    "        if provider == 'google':\n",
    "            api_key = os.getenv(\"GOOGLE_API_KEY\")\n",
    "            model_name = os.getenv(\"GOOGLE_MODEL_NAME\", \"gemini-1.5-flash-latest\")\n",
    "            if not api_key:\n",
    "                logging.error(\"GOOGLE_API_KEY not found in environment variables.\")\n",
    "                return None\n",
    "            logging.info(f\"Using Google AI model: {model_name}\")\n",
    "            return ChatGoogleGenerativeAI(model=model_name, google_api_key=api_key)\n",
    "\n",
    "        elif provider == 'nvidia':\n",
    "            api_key = os.getenv(\"NVIDIA_API_KEY\")\n",
    "            base_url = os.getenv(\"NVIDIA_BASE_URL\")\n",
    "            model_name = os.getenv(\"NVIDIA_MODEL_NAME\") # Required for NIM\n",
    "\n",
    "            if not api_key:\n",
    "                logging.error(\"NVIDIA_API_KEY not found.\")\n",
    "                return None\n",
    "            if not base_url:\n",
    "                 logging.error(\"NVIDIA_BASE_URL (NIM endpoint invoke URL base) not found.\")\n",
    "                 return None\n",
    "            if not model_name:\n",
    "                logging.error(\"NVIDIA_MODEL_NAME (specific model served by NIM) not found.\")\n",
    "                return None\n",
    "\n",
    "            # Ensure base_url doesn't end with common suffixes like /v1/chat/completions\n",
    "            if base_url.endswith(\"/v1/chat/completions\"):\n",
    "                 base_url = base_url[:-len(\"/v1/chat/completions\")]\n",
    "            elif base_url.endswith(\"/\"):\n",
    "                 base_url = base_url[:-1]\n",
    "\n",
    "            logging.info(f\"Using NVIDIA NIM model: {model_name} at {base_url}\")\n",
    "            # Note: ChatNVIDIA uses base_url slightly differently than raw API calls\n",
    "            # It expects the base path to the API, often ending in /v1\n",
    "            # Adjust if necessary based on langchain-nvidia-ai-endpoints documentation or testing\n",
    "            # For NIM, often just the base invoke URL works if model is specified.\n",
    "            return ChatNVIDIA(\n",
    "                base_url=base_url,\n",
    "                api_key=api_key,\n",
    "                model=model_name # Specify the model served by the NIM endpoint\n",
    "            )\n",
    "\n",
    "\n",
    "        elif provider == 'ollama':\n",
    "            model_name = os.getenv(\"OLLAMA_MODEL\", \"llama3\")\n",
    "            base_url = os.getenv(\"OLLAMA_BASE_URL\", \"http://localhost:11434\")\n",
    "            logging.info(f\"Using Ollama model: {model_name} at {base_url}\")\n",
    "            # Check if Ollama server is reachable (optional but good practice)\n",
    "            try:\n",
    "                # Simple check, replace with more robust health check if needed\n",
    "                import requests\n",
    "                requests.get(f\"{base_url}/api/tags\", timeout=5).raise_for_status()\n",
    "            except Exception as e:\n",
    "                 logging.error(f\"Could not connect to Ollama at {base_url}. Is it running? Error: {e}\")\n",
    "                 return None\n",
    "            return ChatOllama(model=model_name, base_url=base_url)\n",
    "\n",
    "        else:\n",
    "            logging.error(f\"Invalid provider specified: {provider}\")\n",
    "            return None\n",
    "    except ImportError as e:\n",
    "        logging.error(f\"Missing library for provider '{provider}': {e}. Please install required packages.\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to initialize LLM for provider '{provider}': {e}\")\n",
    "        return None\n",
    "\n",
    "# --- Summarization Logic ---\n",
    "\n",
    "def summarize_website(url: str, provider: Literal['google', 'nvidia', 'ollama']) -> Optional[str]:\n",
    "    \"\"\"\n",
    "    Fetches content from a URL and summarizes it using the specified LLM provider.\n",
    "    \"\"\"\n",
    "    logging.info(f\"Attempting to summarize URL: {url} using provider: {provider}\")\n",
    "\n",
    "    llm = get_llm(provider)\n",
    "    if not llm:\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        # 1. Load Website Content\n",
    "        # WebBaseLoader handles fetching and basic parsing\n",
    "        # You might need to install 'html2text' and 'beautifulsoup4'\n",
    "        loader = WebBaseLoader(url)\n",
    "        docs = loader.load() # Returns a list of Document objects (usually one)\n",
    "        logging.info(f\"Successfully loaded content from {url}. Content length: {len(docs[0].page_content)} chars.\")\n",
    "\n",
    "        if not docs or not docs[0].page_content:\n",
    "             logging.warning(f\"No content extracted from {url}.\")\n",
    "             return \"Could not extract content from the website.\"\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to load URL {url}: {e}\")\n",
    "        return f\"Error loading URL: {e}\"\n",
    "\n",
    "    try:\n",
    "        # 2. Summarization Chain\n",
    "        # Using map_reduce: Good for longer documents. It summarizes chunks independently (map)\n",
    "        # then combines those summaries (reduce).\n",
    "        # Other options: 'stuff' (for short docs), 'refine' (iterative refinement)\n",
    "        chain = load_summarize_chain(llm, chain_type=\"map_reduce\")\n",
    "\n",
    "        # Optional: Define custom prompts for map and combine steps\n",
    "        # map_prompt_template = \"Summarize the following text:\\n{text}\\nSummary:\"\n",
    "        # combine_prompt_template = \"Combine the following summaries:\\n{text}\\nCombined Summary:\"\n",
    "        # map_prompt = ChatPromptTemplate.from_template(map_prompt_template)\n",
    "        # combine_prompt = ChatPromptTemplate.from_template(combine_prompt_template)\n",
    "        # chain = load_summarize_chain(\n",
    "        #     llm,\n",
    "        #     chain_type=\"map_reduce\",\n",
    "        #     map_prompt=map_prompt,\n",
    "        #     combine_prompt=combine_prompt\n",
    "        # )\n",
    "\n",
    "        logging.info(\"Starting summarization process...\")\n",
    "        summary_result = chain.invoke(docs) # Pass the list of documents\n",
    "\n",
    "        # The result structure might vary slightly depending on the chain type\n",
    "        summary = summary_result.get('output_text', 'Summarization failed.')\n",
    "\n",
    "        logging.info(f\"Summarization complete for {url}.\")\n",
    "        return summary\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Summarization failed for {url} using {provider}: {e}\")\n",
    "        # Consider more specific error handling for API errors (rate limits, auth)\n",
    "        return f\"Error during summarization: {e}\"\n",
    "\n",
    "# --- Main Execution ---\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # --- Configuration ---\n",
    "    # List of websites to summarize daily\n",
    "    WEBSITES_TO_SUMMARIZE = [\n",
    "        \"https://blog.langchain.dev/langchain-expression-language/\",\n",
    "        \"https://developer.nvidia.com/blog/\",\n",
    "        # Add more URLs here\n",
    "    ]\n",
    "\n",
    "    # Choose your preferred provider for this run\n",
    "    # Can be dynamically chosen or configured elsewhere\n",
    "    # PROVIDER_CHOICE: Literal['google', 'nvidia', 'ollama'] = 'google'\n",
    "    # PROVIDER_CHOICE: Literal['google', 'nvidia', 'ollama'] = 'nvidia'\n",
    "    PROVIDER_CHOICE: Literal['google', 'nvidia', 'ollama'] = 'ollama'\n",
    "\n",
    "    # --- Summarization Loop ---\n",
    "    all_summaries = {}\n",
    "    for site_url in WEBSITES_TO_SUMMARIZE:\n",
    "        summary = summarize_website(site_url, PROVIDER_CHOICE)\n",
    "        if summary:\n",
    "            print(\"-\" * 80)\n",
    "            print(f\"Summary for: {site_url} (using {PROVIDER_CHOICE})\")\n",
    "            print(\"-\" * 80)\n",
    "            print(summary)\n",
    "            print(\"\\n\")\n",
    "            all_summaries[site_url] = summary\n",
    "        else:\n",
    "            print(\"-\" * 80)\n",
    "            print(f\"Failed to get summary for: {site_url} (using {PROVIDER_CHOICE})\")\n",
    "            print(\"-\" * 80)\n",
    "            all_summaries[site_url] = \"Failed\"\n",
    "\n",
    "    # --- Optional: Do something with the summaries ---\n",
    "    # e.g., save to a file, send an email, etc.\n",
    "    # with open(\"daily_summaries.txt\", \"w\") as f:\n",
    "    #     for url, summary_text in all_summaries.items():\n",
    "    #         f.write(f\"URL: {url}\\nProvider: {PROVIDER_CHOICE}\\nSummary:\\n{summary_text}\\n\\n{'='*80}\\n\\n\")\n",
    "    # logging.info(\"Summaries saved to daily_summaries.txt\")\n",
    "\n",
    "\n",
    "# Run website summarizer every day at 7 AM\n",
    "# 0 7 * * * /usr/bin/python3 /path/to/your/website_summarizer.py >> /path/to/your/summarizer.log 2>&1"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
